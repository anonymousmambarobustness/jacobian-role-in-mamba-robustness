import re
import torch
import torch.nn as nn
import time
import torch.nn.functional as F
import numpy as np
from scipy import stats as st
import random
from torch.autograd import Variable
class Attack(object):
    r"""
    Base class for all attacks.

    .. note::
        It automatically set device to the device where given model is.
        It basically changes training mode to eval during attack process.
        To change this, please see `set_training_mode`.
    """
    def __init__(self, name, model):
        r"""
        Initializes internal attack state.

        Arguments:
            name (str): name of attack.
            model (torch.nn.Module): model to attack.
        """

        self.attack = name
        self.model = model
        self.model_name = str(model).split("(")[0]
        self.device = next(model.parameters()).device

        self._attack_mode = 'default'
        self._targeted = False
        self._return_type = 'float'
        self._supported_mode = ['default']

        self._model_training = False
        self._batchnorm_training = False
        self._dropout_training = False

    def forward(self, *input):
        r"""
        It defines the computation performed at every call.
        Should be overridden by all subclasses.
        """
        raise NotImplementedError

    def get_mode(self):
        r"""
        Get attack mode.

        """
        return self._attack_mode

    def set_mode_default(self):
        r"""
        Set attack mode as default mode.

        """
        self._attack_mode = 'default'
        self._targeted = False
        print("Attack mode is changed to 'default.'")

    def set_mode_targeted_by_function(self, target_map_function=None):
        r"""
        Set attack mode as targeted.

        Arguments:
            target_map_function (function): Label mapping function.
                e.g. lambda images, labels:(labels+1)%10.
                None for using input labels as targeted labels. (Default)

        """
        if "targeted" not in self._supported_mode:
            raise ValueError("Targeted mode is not supported.")

        self._attack_mode = 'targeted'
        self._targeted = True
        self._target_map_function = target_map_function
        print("Attack mode is changed to 'targeted.'")

    def set_mode_targeted_least_likely(self, kth_min=1):
        r"""
        Set attack mode as targeted with least likely labels.
        Arguments:
            kth_min (str): label with the k-th smallest probability used as target labels. (Default: 1)

        """
        if "targeted" not in self._supported_mode:
            raise ValueError("Targeted mode is not supported.")

        self._attack_mode = "targeted(least-likely)"
        self._targeted = True
        assert (kth_min > 0)
        self._kth_min = kth_min
        self._target_map_function = self._get_least_likely_label
        print("Attack mode is changed to 'targeted(least-likely).'")

    def set_mode_targeted_random(self):
        r"""
        Set attack mode as targeted with random labels.
        Arguments:
            num_classses (str): number of classes.

        """
        if "targeted" not in self._supported_mode:
            raise ValueError("Targeted mode is not supported.")

        self._attack_mode = "targeted(random)"
        self._targeted = True
        self._target_map_function = self._get_random_target_label
        print("Attack mode is changed to 'targeted(random).'")

    def set_return_type(self, type):
        r"""
        Set the return type of adversarial images: `int` or `float`.

        Arguments:
            type (str): 'float' or 'int'. (Default: 'float')

        .. note::
            If 'int' is used for the return type, the file size of 
            adversarial images can be reduced (about 1/4 for CIFAR10).
            However, if the attack originally outputs float adversarial images
            (e.g. using small step-size than 1/255), it might reduce the attack
            success rate of the attack.

        """
        if type == 'float':
            self._return_type = 'float'
        elif type == 'int':
            self._return_type = 'int'
        else:
            raise ValueError(type + " is not a valid type. [Options: float, int]")

    def set_training_mode(self, model_training=False, batchnorm_training=False, dropout_training=False):
        r"""
        Set training mode during attack process.

        Arguments:
            model_training (bool): True for using training mode for the entire model during attack process.
            batchnorm_training (bool): True for using training mode for batchnorms during attack process.
            dropout_training (bool): True for using training mode for dropouts during attack process.

        .. note::
            For RNN-based models, we cannot calculate gradients with eval mode.
            Thus, it should be changed to the training mode during the attack.
        """
        self._model_training = model_training
        self._batchnorm_training = batchnorm_training
        self._dropout_training = dropout_training

    def save(self, data_loader, save_path=None, verbose=True, return_verbose=False, save_pred=False):
        r"""
        Save adversarial images as torch.tensor from given torch.utils.data.DataLoader.

        Arguments:
            save_path (str): save_path.
            data_loader (torch.utils.data.DataLoader): data loader.
            verbose (bool): True for displaying detailed information. (Default: True)
            return_verbose (bool): True for returning detailed information. (Default: False)
            save_pred (bool): True for saving predicted labels (Default: False)

        """
        if save_path is not None:
            image_list = []
            label_list = []
            if save_pred:
                pre_list = []

        correct = 0
        total = 0
        l2_distance = []

        total_batch = len(data_loader)

        given_training = self.model.training
        given_return_type = self._return_type
        self._return_type = 'float'

        for step, (images, labels) in enumerate(data_loader):
            start = time.time()
            adv_images = self.__call__(images, labels)

            batch_size = len(images)

            if verbose or return_verbose:
                with torch.no_grad():
                    if given_training:
                        self.model.eval()
                    outputs = self.model(adv_images)
                    _, pred = torch.max(outputs.data, 1)
                    total += labels.size(0)
                    right_idx = (pred == labels.to(self.device))
                    correct += right_idx.sum()
                    end = time.time()
                    delta = (adv_images - images.to(self.device)).view(batch_size, -1)
                    l2_distance.append(torch.norm(delta[~right_idx], p=2, dim=1))

                    rob_acc = 100 * float(correct) / total
                    l2 = torch.cat(l2_distance).mean().item()
                    progress = (step+1)/total_batch*100
                    elapsed_time = end-start
                    if verbose:
                        self._save_print(progress, rob_acc, l2, elapsed_time, end='\r')

            if save_path is not None:
                if given_return_type == 'int':
                    adv_images = self._to_uint(adv_images.detach().cpu())
                    image_list.append(adv_images)
                else:
                    image_list.append(adv_images.detach().cpu())
                label_list.append(labels.detach().cpu())

                image_list_cat = torch.cat(image_list, 0)
                label_list_cat = torch.cat(label_list, 0)

                if save_pred:
                    pre_list.append(pred.detach().cpu())
                    pre_list_cat = torch.cat(pre_list, 0)
                    torch.save((image_list_cat, label_list_cat, pre_list_cat), save_path)
                else:
                    torch.save((image_list_cat, label_list_cat), save_path)

        # To avoid erasing the printed information.
        if verbose:
            self._save_print(progress, rob_acc, l2, elapsed_time, end='\n')

        if given_training:
            self.model.train()

        if return_verbose:
            return rob_acc, l2, elapsed_time

    def _save_print(self, progress, rob_acc, l2, elapsed_time, end):
        print('- Save progress: %2.2f %% / Robust accuracy: %2.2f %% / L2: %1.5f (%2.3f it/s) \t' \
              % (progress, rob_acc, l2, elapsed_time), end=end)

    @torch.no_grad()
    def _get_target_label(self, images, labels=None):
        r"""
        Function for changing the attack mode.
        Return input labels.
        """
        if self._targeted:
            given_training = self.model.training
            if given_training:
                self.model.eval()
            target_labels = self._target_map_function(images, labels)
            if given_training:
                self.model.train()
            return target_labels
        else:
            raise ValueError('Please define target_map_function.')

    @torch.no_grad()
    def _get_least_likely_label(self, images, labels=None):
        outputs = self.model(images)
        if labels is None:
            _, labels = torch.max(outputs, dim=1)
        n_classses = outputs.shape[-1]

        target_labels = torch.zeros_like(labels)
        for counter in range(labels.shape[0]):
            l = list(range(n_classses))
            l.remove(labels[counter])
            _, t = torch.kthvalue(outputs[counter][l], self._kth_min)
            target_labels[counter] = l[t]

        return target_labels.long().to(self.device)

    @torch.no_grad()
    def _get_random_target_label(self, images, labels=None):
        outputs = self.model(images)
        if labels is None:
            _, labels = torch.max(outputs, dim=1)
        n_classses = outputs.shape[-1]

        target_labels = torch.zeros_like(labels)
        for counter in range(labels.shape[0]):
            l = list(range(n_classses))
            l.remove(labels[counter])
            t = (len(l)*torch.rand([1])).long().to(self.device)
            target_labels[counter] = l[t]

        return target_labels.long().to(self.device)

    def _to_uint(self, images):
        r"""
        Function for changing the return type.
        Return images as int.
        """
        return (images*255).type(torch.uint8)

    def __str__(self):
        info = self.__dict__.copy()

        del_keys = ['model', 'attack']

        for key in info.keys():
            if key[0] == "_":
                del_keys.append(key)

        for key in del_keys:
            del info[key]

        info['attack_mode'] = self._attack_mode
        info['return_type'] = self._return_type

        return self.attack + "(" + ', '.join('{}={}'.format(key, val) for key, val in info.items()) + ")"

    def __call__(self, *input, **kwargs):
        given_training = self.model.training

        if self._model_training:
            self.model.train()
            for _, m in self.model.named_modules():
                if not self._batchnorm_training:
                    if 'BatchNorm' in m.__class__.__name__:
                        m = m.eval()
                if not self._dropout_training:
                    if 'Dropout' in m.__class__.__name__:
                        m = m.eval()
        else:
            self.model.eval()

        images = self.forward(*input, **kwargs)

        if given_training:
            self.model.train()

        if self._return_type == 'int':
            images = self._to_uint(images)

        return images

class MIFGSM(Attack):
    r"""
    MI-FGSM in the paper 'Boosting Adversarial Attacks with Momentum'
    [https://arxiv.org/abs/1710.06081]

    Distance Measure : Linf

    Arguments:
        model (nn.Module): model to attack.
        eps (float): maximum perturbation. (Default: 8/255)
        alpha (float): step size. (Default: 2/255)
        decay (float): momentum factor. (Default: 1.0)
        steps (int): number of iterations. (Default: 5)

    Shape:
        - images: :math:`(N, C, H, W)` where `N = number of batches`, `C = number of channels`,        `H = height` and `W = width`. It must have a range [0, 1].
        - labels: :math:`(N)` where each value :math:`y_i` is :math:`0 \leq y_i \leq` `number of labels`.
        - output: :math:`(N, C, H, W)`.

    Examples::
        >>> attack = torchattacks.MIFGSM(model, eps=8/255, steps=5, decay=1.0)
        >>> adv_images = attack(images, labels)

    """

    def __init__(self, model, eps=8/255, alpha=2/255, steps=5, decay=1.0,loss=nn.CrossEntropyLoss(),targeted=False, target=None, normalize = True):
        super().__init__("MIFGSM", model)
        self.eps = eps
        self.steps = steps
        self.decay = decay
        self.alpha = alpha
        self.loss = loss
        self.targeted = targeted
        self.target = target
        self.normalize = normalize
    def norm(self, t):
        mean = [0.4914, 0.4822, 0.4465]
        std = [0.2023, 0.1994, 0.2010]
        t[:, 0, :, :] = (t[:, 0, :, :] - mean[0])/std[0]
        t[:, 1, :, :] = (t[:, 1, :, :] - mean[1])/std[1]
        t[:, 2, :, :] = (t[:, 2, :, :] - mean[2])/std[2]

        return t
    def forward(self, images, labels):
        r"""
        Overridden.
        """
        images = images.clone().detach().to(self.device)
        labels = labels.clone().detach().to(self.device)

        if self.targeted:
            labels = self.target*torch.ones_like(labels)

        momentum = torch.zeros_like(images).detach().to(self.device)

        loss = self.loss

        adv_delta = torch.zeros_like(images).to(self.device)
        
        clean_images = images.clone().detach()

        for _ in range(self.steps):
            #adv_images.requires_grad = True
            adv_delta.requires_grad = True
            adv_images = torch.clamp(clean_images+adv_delta, min=0, max=1)
            if self.normalize:
                outputs = self.model(self.norm(adv_images))
            else:
                outputs = self.model(adv_images)
            # Calculate loss
            if self.targeted:
                cost = -loss(outputs, labels)
            else:
                cost = loss(outputs, labels)

            # Update adversarial images
            grad = torch.autograd.grad(cost, adv_delta,
                                       retain_graph=False, create_graph=False)[0]

            grad = grad / torch.mean(torch.abs(grad), dim=(1,2,3), keepdim=True)
            grad = grad + momentum*self.decay
            momentum = grad

            adv_delta = adv_delta.detach() + self.alpha*grad.sign()
            adv_delta = torch.clamp(adv_delta, min=-self.eps, max=self.eps)
        adv_images = torch.clamp(images + adv_delta, min=0, max=1).detach()

        return adv_images

class TIFGSM(Attack):
    r"""
    TIFGSM in the paper 'Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks'
    [https://arxiv.org/abs/1904.02884]

    Distance Measure : Linf

    Arguments:
        model (nn.Module): model to attack.
        eps (float): maximum perturbation. (Default: 8/255)
        alpha (float): step size. (Default: 2/255)
        steps (int): number of iterations. (Default: 20)
        decay (float): momentum factor. (Default: 0.0)
        kernel_name (str): kernel name. (Default: gaussian)
        len_kernel (int): kernel length.  (Default: 15, which is the best according to the paper)
        nsig (int): radius of gaussian kernel. (Default: 3; see Section 3.2.2 in the paper for explanation)
        resize_rate (float): resize factor used in input diversity. (Default: 0.9)
        diversity_prob (float) : the probability of applying input diversity. (Default: 0.5)
        random_start (bool): using random initialization of delta. (Default: False)

    Shape:
        - images: :math:`(N, C, H, W)` where `N = number of batches`, `C = number of channels`, `H = height` and `W = width`. It must have a range [0, 1].
        - labels: :math:`(N)` where each value :math:`y_i` is :math:`0 \leq y_i \leq` `number of labels`.
        - output: :math:`(N, C, H, W)`.

    Examples::
        >>> attack = torchattacks.TIFGSM(model, eps=8/255, alpha=2/255, steps=20, decay=1.0, resize_rate=0.9, diversity_prob=0.7, random_start=False)
        >>> adv_images = attack(images, labels)

    """
    def __init__(self, model, eps=8/255, alpha=2/255, steps=20, decay=1.0, kernel_name='gaussian',
                 len_kernel=5, nsig=3, resize_rate=0.9, diversity_prob=0.7, random_start=False,
                 loss=nn.CrossEntropyLoss(),targeted=False, target=None,normalize=True):
        super().__init__("TIFGSM", model)
        self.eps = eps
        self.steps = steps
        self.decay = decay
        self.alpha = alpha
        self.resize_rate = resize_rate
        self.diversity_prob = diversity_prob
        self.random_start = random_start
        self.kernel_name = kernel_name
        self.len_kernel = len_kernel
        self.nsig = nsig
        self.stacked_kernel = torch.from_numpy(self.kernel_generation())
        self.loss = loss
        self.targeted = targeted
        self.target = target
        self.normalize = normalize
    def norm(self, t):
        mean = [0.4914, 0.4822, 0.4465]
        std = [0.2023, 0.1994, 0.2010]
        t[:, 0, :, :] = (t[:, 0, :, :] - mean[0])/std[0]
        t[:, 1, :, :] = (t[:, 1, :, :] - mean[1])/std[1]
        t[:, 2, :, :] = (t[:, 2, :, :] - mean[2])/std[2]

        return t
    def forward(self, images, labels):
        r"""
        Overridden.
        """
        images = images.clone().detach().to(self.device)
        labels = labels.clone().detach().to(self.device)

        if self.targeted:
            labels = self.target*torch.ones_like(labels)

        loss = self.loss
        momentum = torch.zeros_like(images).detach().to(self.device)
        stacked_kernel = self.stacked_kernel.to(self.device)

        adv_delta = torch.zeros_like(images).to(self.device)
        
        clean_images = images.clone().detach()

        if self.random_start:
            # Starting at a uniformly random point
            adv_delta = adv_delta + torch.empty_like(adv_delta).uniform_(-self.eps, self.eps)
            clean_images = torch.clamp(clean_images+adv_delta, min=0, max=1).detach()

        for _ in range(self.steps):
            #adv_images.requires_grad = True
            adv_delta.requires_grad = True
            adv_images = torch.clamp(clean_images+adv_delta, min=0, max=1)
            if self.normalize:
                outputs = self.model(self.norm(self.input_diversity(adv_images)))
            else:
                outputs = self.model(self.input_diversity(adv_images))

            # Calculate loss
            if self.targeted:
                cost = -loss(outputs, labels)
            else:
                cost = loss(outputs, labels)

            # Update adversarial images
            grad = torch.autograd.grad(cost, adv_delta,
                                       retain_graph=False, create_graph=False)[0]
            # depth wise conv2d
            grad = F.conv2d(grad, stacked_kernel, stride=1, padding='same', groups=3)
            grad = grad / torch.mean(torch.abs(grad), dim=(1,2,3), keepdim=True)
            grad = grad + momentum*self.decay
            momentum = grad

            adv_delta = adv_delta.detach() + self.alpha*grad.sign()
            adv_delta = torch.clamp(adv_delta, min=-self.eps, max=self.eps)
        adv_images = torch.clamp(images + adv_delta, min=0, max=1).detach()

        return adv_images

    def kernel_generation(self):
        if self.kernel_name == 'gaussian':
            kernel = self.gkern(self.len_kernel, self.nsig).astype(np.float32)
        elif self.kernel_name == 'linear':
            kernel = self.lkern(self.len_kernel).astype(np.float32)
        elif self.kernel_name == 'uniform':
            kernel = self.ukern(self.len_kernel).astype(np.float32)
        else:
            raise NotImplementedError

        stack_kernel = np.stack([kernel, kernel, kernel])
        stack_kernel = np.expand_dims(stack_kernel, 1)
        return stack_kernel

    def gkern(self, kernlen=15, nsig=3):
        """Returns a 2D Gaussian kernel array."""
        x = np.linspace(-nsig, nsig, kernlen)
        kern1d = st.norm.pdf(x)
        kernel_raw = np.outer(kern1d, kern1d)
        kernel = kernel_raw / kernel_raw.sum()
        return kernel

    def ukern(self, kernlen=15):
        kernel = np.ones((kernlen,kernlen))* 1.0 /(kernlen*kernlen)
        return kernel

    def lkern(self, kernlen=15):
        kern1d = 1-np.abs(np.linspace((-kernlen+1)/2, (kernlen-1)/2, kernlen)/(kernlen+1)*2)
        kernel_raw = np.outer(kern1d, kern1d)
        kernel = kernel_raw / kernel_raw.sum()
        return kernel

    def input_diversity(self, x):
        img_size = x.shape[-1]
        img_resize = int(img_size * self.resize_rate)

        if self.resize_rate < 1:
            img_size = img_resize
            img_resize = x.shape[-1]

        rnd = torch.randint(low=img_size, high=img_resize, size=(1,), dtype=torch.int32)
        rescaled = F.interpolate(x, size=[rnd, rnd], mode='bilinear', align_corners=False)
        h_rem = img_resize - rnd
        w_rem = img_resize - rnd
        pad_top = torch.randint(low=0, high=h_rem.item(), size=(1,), dtype=torch.int32)
        pad_bottom = h_rem - pad_top
        pad_left = torch.randint(low=0, high=w_rem.item(), size=(1,), dtype=torch.int32)
        pad_right = w_rem - pad_left

        padded = F.pad(rescaled, [pad_left.item(), pad_right.item(), pad_top.item(), pad_bottom.item()], value=0)

        return padded if torch.rand(1) < self.diversity_prob else x


class PGD(Attack):
    r"""
    PGD in the paper 'Towards Deep Learning Models Resistant to Adversarial Attacks'
    [https://arxiv.org/abs/1706.06083]

    Distance Measure : Linf

    Arguments:
        model (nn.Module): model to attack.
        eps (float): maximum perturbation. (Default: 0.3)
        alpha (float): step size. (Default: 2/255)
        steps (int): number of steps. (Default: 40)
        random_start (bool): using random initialization of delta. (Default: True)

    Shape:
        - images: :math:`(N, C, H, W)` where `N = number of batches`, `C = number of channels`,        `H = height` and `W = width`. It must have a range [0, 1].
        - labels: :math:`(N)` where each value :math:`y_i` is :math:`0 \leq y_i \leq` `number of labels`.
        - output: :math:`(N, C, H, W)`.

    Examples::
        >>> attack = torchattacks.PGD(model, eps=8/255, alpha=1/255, steps=40, random_start=True)
        >>> adv_images = attack(images, labels)

    """
    def __init__(self, model, eps=16/255,
                 alpha=2/255, steps=40, random_start=True,loss=nn.CrossEntropyLoss(),targeted=False, target=None, normalize = True , mean=None, std=None):
        super().__init__("PGD", model)
        self.eps = eps
        self.alpha = alpha
        self.steps = steps
        self.random_start = random_start
        self._supported_mode = ['default', 'targeted']
        self.loss = loss
        self.targeted = targeted
        self.target = target
        self.normalize = normalize
        self.mean = mean
        self.std = std
    def norm(self, t):
        if self.mean is None and self.std is None:
            mean = [0.4914, 0.4822, 0.4465]
            std = [0.2023, 0.1994, 0.2010]
        else:
            mean = self.mean
            std = self.std
        print(f"PGD: norm: mean={mean} std={std}")
        t[:, 0, :, :] = (t[:, 0, :, :] - mean[0])/std[0]
        t[:, 1, :, :] = (t[:, 1, :, :] - mean[1])/std[1]
        t[:, 2, :, :] = (t[:, 2, :, :] - mean[2])/std[2]

        return t
    def forward(self, images, labels):
        r"""
        Overridden.
        """
        images = images.clone().detach().to(self.device)
        labels = labels.clone().detach().to(self.device)

        if self.targeted:
            labels = self.target*torch.ones_like(labels)

        loss = self.loss

        adv_delta = torch.zeros_like(images).to(self.device)
        
        clean_images = images.clone().detach()

        if self.random_start:
            # Starting at a uniformly random point
            adv_delta = adv_delta + torch.empty_like(adv_delta).uniform_(-self.eps, self.eps)
            clean_images = torch.clamp(clean_images+adv_delta, min=0, max=1).detach()

        for _ in range(self.steps):
            adv_delta.requires_grad = True
            adv_images = torch.clamp(clean_images+adv_delta, min=0, max=1)
            if self.normalize:
                outputs = self.model(self.norm(adv_images))
            else:
                outputs = self.model(adv_images)

            # Calculate loss
            if self.targeted:
                cost = -loss(outputs, labels)
            else:
                cost = loss(outputs, labels)
            # Update adversarial images
            grad = torch.autograd.grad(cost, adv_delta, retain_graph=False, create_graph=False)[0]
            adv_delta = adv_delta.detach() + self.alpha*grad.sign()
            adv_delta = torch.clamp(adv_delta, min=-self.eps, max=self.eps)
        adv_images = torch.clamp(images + adv_delta, min=0, max=1).detach()

        return adv_images



class TestTimeJacobian(Attack):
    def __init__(self, model, eps=16 / 255,
                 alpha=2 / 255, steps=40, random_start=True, loss=nn.CrossEntropyLoss(), targeted=False,
                 target=None, normalize=True, reg_type="gp_correct_class_l2", use_grad_value=False, use_delta_as_jacobian_input=False):
        super().__init__("TestTimeJacobian", model)
        self.eps = eps
        self.alpha = alpha
        self.steps = steps
        self.random_start = random_start
        self._supported_mode = ['default', 'targeted']
        self.loss = loss
        self.targeted = targeted
        self.target = target
        self.normalize = normalize
        self.reg_type = reg_type
        self.use_grad_value = use_grad_value
        self.use_delta_as_jacobian_input = use_delta_as_jacobian_input

    def norm(self, t):
        mean = [0.4914, 0.4822, 0.4465]
        std = [0.2023, 0.1994, 0.2010]
        t[:, 0, :, :] = (t[:, 0, :, :] - mean[0]) / std[0]
        t[:, 1, :, :] = (t[:, 1, :, :] - mean[1]) / std[1]
        t[:, 2, :, :] = (t[:, 2, :, :] - mean[2]) / std[2]

        return t

    def compute_jacobian_correct_class(self, inputs, outputs, targets):
        batch_size = inputs.shape[0]
        output_dim = outputs.shape[-1]
        cotangents = torch.zeros(batch_size, output_dim, device=inputs.device)
        cotangents[range(batch_size), targets] = 1.0
        jacobian_row = torch.autograd.grad(
            outputs=outputs,
            inputs=inputs,
            grad_outputs=cotangents,
            create_graph=True,
            retain_graph=True,
        )[0]
        return jacobian_row


    def compute_gp_max_output(self, inputs, outputs):
        batch_size = inputs.shape[0]
        output_dim = outputs.shape[-1]
        max_indices = torch.argmax(outputs, dim=1)
        cotangents = torch.zeros(batch_size, output_dim, device=inputs.device)
        cotangents[range(batch_size), max_indices] = 1.0
        jacobian_row = torch.autograd.grad(
            outputs=outputs,
            inputs=inputs,
            grad_outputs=cotangents,
            create_graph=True,
            retain_graph=True,
        )[0]
        return jacobian_row


    def compute_jacobian_norm(self, jacobian_row, norm_type):
        batch_size = jacobian_row.shape[0]
        print(f"compute_jacobian_norm: batch_size={batch_size}")
        if norm_type == "l1":
            jacobian_norm = jacobian_row.view(batch_size, -1).norm(1, 1) # (p=1,dim=1)
        elif norm_type == "l2":
            jacobian_norm = jacobian_row.view(batch_size, -1).norm(2, 1)
        elif norm_type == "l_inf":
            jacobian_norm = jacobian_row.view(batch_size, -1).abs().max(dim=1)[0]  # L-infinity norm
        elif norm_type == "l2_squared":
            jacobian_l2_norm = jacobian_row.view(batch_size, -1).norm(2, 1)
            jacobian_norm = jacobian_l2_norm**2
        else:
            jacobian_norm = None
            print(f"norm is not supported! -> exit program!")
            exit(1)
        return torch.mean(jacobian_norm)


    def get_model_jacobian_norm(self, inputs, outputs, targets, reg_type):
        print(f"get_model_jacobian_norm: DEBUG: reg_type={reg_type}")
        match = re.match(r"(.+)_l(inf|2_squared|\d+)$", reg_type)
        reg_type, norm_type = match.groups()
        norm_type = f"l{norm_type}"
        print(f"DEBUG: get_model_jacobian_norm: reg_type={reg_type} norm_type={norm_type}")
        if reg_type == "gp_correct_class":
            jacobian_row = self.compute_jacobian_correct_class(inputs, outputs, targets)
        elif reg_type == "gp_max_output":
            jacobian_row = self.compute_gp_max_output(inputs, outputs)
        else:
            jacobian_row = None
            print(f"ERROR: invalid regularization type: {reg_type} -> exit program")
            exit(1)
        print(f"DEBUG: get_model_jacobian_norm: jacobian_row.shape={jacobian_row.shape}")
        jacobian_norm = self.compute_jacobian_norm(jacobian_row, norm_type)
        print(f"DEBUG: get_model_jacobian_norm: jacobian_norm={jacobian_norm}")
        return jacobian_norm


    def forward(self, images, labels):
        r"""
        Overridden.
        """
        images = images.clone().detach().to(self.device)
        labels = labels.clone().detach().to(self.device)

        if self.targeted:
            labels = self.target * torch.ones_like(labels)

        loss = self.loss

        adv_delta = torch.zeros_like(images).to(self.device)

        clean_images = images.clone().detach()

        if self.random_start:
            # Starting at a uniformly random point
            adv_delta = adv_delta + torch.empty_like(adv_delta).uniform_(-self.eps, self.eps)
            clean_images = torch.clamp(clean_images + adv_delta, min=0, max=1).detach()

        for _ in range(self.steps):
            adv_delta.requires_grad = True
            adv_images = torch.clamp(clean_images + adv_delta, min=0, max=1)
            if self.normalize:
                outputs = self.model(self.norm(adv_images))
            else:
                outputs = self.model(adv_images)

            # Calculate loss
            if self.targeted:
                print(f"targeted=True!!!!!")
                #cost = -self.get_model_jacobian_norm(inputs=adv_delta, outputs=outputs, targets=labels, reg_type=self.reg_type) if self.use_delta_as_jacobian_input else -self.get_model_jacobian_norm(inputs=adv_images, outputs=outputs, targets=labels, reg_type=self.reg_type)
            else:
                #cost = self.get_model_jacobian_norm(inputs=adv_delta, outputs=outputs, targets=labels, reg_type=self.reg_type) if self.use_delta_as_jacobian_input else self.get_model_jacobian_norm(inputs=adv_images, outputs=outputs, targets=labels, reg_type=self.reg_type)
                cost = self.get_model_jacobian_norm(inputs=adv_images, outputs=outputs, targets=labels, reg_type=self.reg_type)
            # Update adversarial images
            grad = torch.autograd.grad(cost, adv_delta, retain_graph=False, create_graph=False)[0]
            if self.use_grad_value:
                adv_delta = adv_delta.detach() - self.alpha * grad
            else:
                adv_delta = adv_delta.detach() - self.alpha * grad.sign()
            adv_delta = torch.clamp(adv_delta, min=-self.eps, max=self.eps)
            adv_delta.requires_grad = False
        adv_images = torch.clamp(images + adv_delta, min=0, max=1).detach()

        return adv_images
